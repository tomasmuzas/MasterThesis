{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "7tHoCzfp75Lk",
        "NIuPexS9cYSO",
        "4CA5sOU70YDc",
        "lXI6DOR8eO4I",
        "6urHfqO36D5L",
        "VYM7sYi7Hp8x"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zjm3ImRshon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8345331a-b668-441d-cb37-61204460d4ff"
      },
      "source": [
        "!git clone https://github.com/tomasmuzas/MasterThesis.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MasterThesis'...\n",
            "remote: Enumerating objects: 111, done.\u001b[K\n",
            "remote: Counting objects: 100% (111/111), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 111 (delta 45), reused 92 (delta 27), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (111/111), 760.25 KiB | 6.61 MiB/s, done.\n",
            "Resolving deltas: 100% (45/45), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHqnkwfts1p0"
      },
      "source": [
        "from MasterThesis.read_dataset import *\n",
        "from MasterThesis.models import *\n",
        "from MasterThesis.callbacks import *\n",
        "from tensorflow.keras import optimizers\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0AGAn9OVgUV"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfgR49dem0gL"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zpvJsa3B4xp"
      },
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_eSKvMi2Awy"
      },
      "source": [
        "def get_dataset():\n",
        "    training_dataset = read_tf_record_dataset_v2(\n",
        "        'gs://tomasmuzasmaster2021/dataset/Training',\n",
        "        tf.keras.layers.Rescaling(scale=1./255),\n",
        "        image_size = 128,\n",
        "        batch_size = 1024,\n",
        "        include_objid = False,\n",
        "        augment = True,\n",
        "        drop_remainder=False,\n",
        "        grayscale=False)\n",
        "    \n",
        "    validation_dataset = read_tf_record_dataset_v2(\n",
        "        'gs://tomasmuzasmaster2021/dataset/Validation',\n",
        "        tf.keras.layers.Rescaling(scale=1./255),\n",
        "        image_size = 128,\n",
        "        batch_size = 16,\n",
        "        include_objid = False,\n",
        "        augment=False,\n",
        "        drop_remainder=True,\n",
        "        grayscale=False)\n",
        "    return (training_dataset.repeat(), 247, validation_dataset.repeat(), 1984)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-TFCUbW4b9Y"
      },
      "source": [
        "import os\n",
        "\n",
        "def perform_training(model_factory, training_parameters, starting_training=0):\n",
        "  tf.keras.backend.clear_session()\n",
        "\n",
        "  model_name = f\"{training_parameters['model_name']}\"\n",
        "  training_dataset, training_steps, validation_dataset, validation_steps = get_dataset()\n",
        "\n",
        "  path = f\"./drive/MyDrive/MTD/Models/{model_name}\"\n",
        "  os.makedirs(path, exist_ok=True)\n",
        "  f = open(path + \"/training_parameters.txt\", \"w\")\n",
        "  f.write(str(training_parameters))\n",
        "  f.close()\n",
        "\n",
        "  for training in range(starting_training, training_parameters[\"trainings\"]):\n",
        "      print(f\"--------------------------------------- TRAINING {training + 1} {training_parameters['model_name']} ---------------------------------------\")\n",
        "      print(\"starting with new model.\")\n",
        "\n",
        "      with strategy.scope():\n",
        "        model = model_factory(training_parameters[\"classes\"], training_parameters[\"image_size\"])\n",
        "        sgd = optimizers.Adam(learning_rate= training_parameters[\"learninig_rate\"], beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "        model.compile(\n",
        "            loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "            optimizer=sgd,\n",
        "            steps_per_execution = 1,\n",
        "            metrics=[tensorflow.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "        callbacks = [\n",
        "          BestAccuracyCallback(training_parameters[\"monitor\"], model_name, f\"training_{training + 1}\"),\n",
        "          # LoggingCallback(model_name, f\"training_{training + 1}\"),\n",
        "          tf.keras.callbacks.EarlyStopping(monitor=training_parameters[\"monitor\"], patience=20, mode='max')\n",
        "        ]\n",
        "\n",
        "        history = model.fit(\n",
        "            x= training_dataset,\n",
        "            validation_data = validation_dataset,\n",
        "            epochs = training_parameters[\"epochs\"],\n",
        "            verbose = 1,\n",
        "            steps_per_epoch = training_steps,\n",
        "            validation_steps = validation_steps,\n",
        "            callbacks= callbacks,\n",
        "            shuffle=True,\n",
        "            class_weight=training_parameters[\"weights\"],\n",
        "            workers=32)\n",
        "\n",
        "        f = open(path + f\"/training_{training + 1}/best.txt\", \"w\")\n",
        "        f.write(str(max(history.history[training_parameters[\"monitor\"]])))\n",
        "        f.close()\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "\n",
        "@tf.function\n",
        "def random_invert_horizontally(x, y, p=0.5):\n",
        "  print(x.shape)\n",
        "  print(y)\n",
        "  print(tf.math.equal(y, 1) == True)\n",
        "  if  tf.random.uniform([]) < p and tf.math.equal(y, 1) == True:\n",
        "    x = tf.image.flip_left_right(x)\n",
        "  else:\n",
        "    x\n",
        "  return x\n",
        "\n",
        "@tf.function\n",
        "def random_invert_vertically(x, y, p=0.5):\n",
        "  if  tf.random.uniform([]) < p and tf.math.equal(y, 1) == True:\n",
        "    x = tf.image.flip_up_down(x)\n",
        "  else:\n",
        "    x\n",
        "  return x\n",
        "\n",
        "@tf.function\n",
        "def random_rotate(x, y, p=0.5):\n",
        "  if  tf.random.uniform([]) < p and tf.math.equal(y, 1) == True:\n",
        "    x = tf.image.rot90(x, k = tf.random.uniform([], minval=1, maxval=4, dtype=tf.int32))\n",
        "  else:\n",
        "    x\n",
        "  return x\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def random_zoom(x, y, p=0.5):\n",
        "  if  tf.random.uniform([]) < p and tf.math.equal(y, 1) == True:\n",
        "    x = tf.image.crop_to_bounding_box(x, 10, 10, 108, 108)\n",
        "    x = tf.image.resize(x, (128,128))\n",
        "  else:\n",
        "    x\n",
        "  return x\n",
        "\n",
        "\n",
        "def read_tf_record_dataset_v2(path, preprocessing_function, image_size, batch_size, augment = False, include_objid = False, drop_remainder = True, grayscale = False):\n",
        "  filenames = tf.io.gfile.glob(path + \"/*.tfrec\")\n",
        "  dataset4 = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "  dataset4 = dataset4.map(lambda x: read_tfrecord(x, image_size), num_parallel_calls=AUTO)\n",
        "  if (include_objid):\n",
        "    dataset4 = dataset4.map(lambda image, class_num, label, objid, one_hot_class: (image, class_num, objid))\n",
        "    dataset4 = dataset4.map(lambda x, y, z: (tf.cast(x, tf.float32), y, z), num_parallel_calls=AUTO)\n",
        "    dataset4 = dataset4.map(lambda x, y, z: (preprocessing_function(x), y, z), num_parallel_calls=AUTO)\n",
        "    if(augment):\n",
        "      dataset4 = dataset4.map(lambda x,y,z : (random_zoom(x, y), y, z), num_parallel_calls=AUTO)\n",
        "      dataset4 = dataset4.map(lambda x,y,z : (random_invert_horizontally(x, y), y, z), num_parallel_calls=AUTO)\n",
        "      dataset4 = dataset4.map(lambda x,y,z : (random_invert_vertically(x, y), y, z), num_parallel_calls=AUTO)\n",
        "      dataset4 = dataset4.map(lambda x,y,z : (random_rotate(x, y), y, z), num_parallel_calls=AUTO)\n",
        "      \n",
        "  else:\n",
        "    dataset4 = dataset4.map(lambda image, class_num, label, objid, one_hot_class: (image, class_num))\n",
        "    dataset4 = dataset4.map(lambda x, y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTO)\n",
        "    dataset4 = dataset4.map(lambda x, y: (preprocessing_function(x), y), num_parallel_calls=AUTO)\n",
        "    if(augment):\n",
        "      dataset4 = dataset4.map(lambda x,y : (random_zoom(x, y), y), num_parallel_calls=AUTO)\n",
        "      dataset4 = dataset4.map(lambda x,y : (random_invert_horizontally(x, y), y), num_parallel_calls=AUTO)\n",
        "      dataset4 = dataset4.map(lambda x,y : (random_invert_vertically(x, y), y), num_parallel_calls=AUTO)\n",
        "      dataset4 = dataset4.map(lambda x,y : (random_rotate(x, y), y), num_parallel_calls=AUTO)\n",
        "      \n",
        "\n",
        "  return dataset4.batch(batch_size, drop_remainder=drop_remainder).prefetch(AUTO)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHloqynWerMX",
        "outputId": "0dd08b54-12b6-4c48-cd7e-e4253c2a1e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxfed67Vc4n0"
      },
      "source": [
        "# All models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXkZaNUw9hQg"
      },
      "source": [
        "from tensorflow.keras.layers import Dropout, Flatten, BatchNormalization\n",
        "\n",
        "NUMBER_OF_CHANNELS = 3\n",
        "\n",
        "def Dieleman(classes, image_size):\n",
        "  model = Sequential(name=\"Dieleman\")\n",
        "  model.add(Input(shape=(image_size, image_size, NUMBER_OF_CHANNELS)))\n",
        "  model.add(Conv2D(filters=32, kernel_size=6, activation='relu'))\n",
        "  model.add(BatchNormalization())   \n",
        "  model.add(MaxPool2D(2))\n",
        "  model.add(Conv2D(filters=64, kernel_size=5, activation='relu'))\n",
        "  model.add(BatchNormalization())  \n",
        "  model.add(MaxPool2D(2))\n",
        "  model.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
        "  model.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(256,activation='relu'))\n",
        "  model.add(Dense(256,activation='relu'))\n",
        "  model.add(Dense(classes, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "\n",
        "def Cavanagh(classes, image_size):\n",
        "  model = Sequential(name=\"Cavanagh\")\n",
        "  model.add(Input(shape=(image_size, image_size, NUMBER_OF_CHANNELS)))\n",
        "  model.add(Conv2D(filters=32, kernel_size=7, activation='relu'))\n",
        "  model.add(BatchNormalization())   \n",
        "  model.add(MaxPool2D(2))\n",
        "  model.add(Conv2D(filters=64, kernel_size=5, activation='relu'))\n",
        "  model.add(Conv2D(filters=64, kernel_size=5, activation='relu'))\n",
        "  model.add(BatchNormalization())  \n",
        "  model.add(MaxPool2D(2))\n",
        "  model.add(Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(256,activation='relu'))\n",
        "  model.add(Dense(256,activation='relu'))\n",
        "  model.add(Dense(classes, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "models = [\n",
        "    {'name': 'Cavanagh', 'func': Cavanagh, 'starting_training': 0},\n",
        "    {'name': 'Dieleman', 'func': Dieleman, 'starting_training': 0},\n",
        "    {'name': 'ResNet50', 'func': create_ResNet50_model, 'starting_training': 0},\n",
        "    {'name': 'InceptionV3', 'func': create_InceptionV3_model, 'starting_training': 0},\n",
        "    {'name': 'InceptionResNetV2', 'func': create_InceptionResNetV2_model, 'starting_training': 0},\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "  training_parameters = {\n",
        "        \"model_name\": \"Models/\" + model[\"name\"],\n",
        "        \"image_size\": 128,\n",
        "        \"learninig_rate\": 1e-4,\n",
        "        \"classes\": 1,\n",
        "        \"weights\": None,\n",
        "        \"epochs\": 500,\n",
        "        \"trainings\": 3,\n",
        "        \"monitor\": 'val_binary_accuracy'\n",
        "    }\n",
        "\n",
        "  perform_training(\n",
        "      model[\"func\"],\n",
        "      training_parameters,\n",
        "      starting_training=model[\"starting_training\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD-ZXdqCG_Sb"
      },
      "source": [
        "# Hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umAAbJgbG7p6",
        "outputId": "25c203bd-2230-4c69-ff8e-97a7902bfbac"
      },
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 19.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 9.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 98 kB 3.4 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vliV3ZnPO3FR",
        "outputId": "c6856840-532e-4b7d-fce5-6609e34d2cb0"
      },
      "source": [
        "import keras_tuner as kt\n",
        "from keras_tuner.applications import HyperResNet\n",
        "from tensorflow.keras.layers import Dropout, Flatten, BatchNormalization\n",
        "\n",
        "def TestModel(hp):\n",
        "  model = Sequential(name=\"Dieleman\")\n",
        "  model.add(Input(shape=(128, 128, 3)))\n",
        "  model.add(Conv2D(filters=hp.Int(\"conv_1_filters\", 16, 32, 16), kernel_size=6, activation='relu'))\n",
        "  model.add(BatchNormalization())   \n",
        "  model.add(MaxPool2D(2))\n",
        "  model.add(Conv2D(filters=hp.Int(\"conv_2_filters\", 16, 64, 16), kernel_size=5, activation='relu'))\n",
        "  model.add(BatchNormalization())  \n",
        "  model.add(MaxPool2D(2))\n",
        "  conv3_filters = hp.Int(\"conv_3_filters\", 32, 128, 16)\n",
        "  model.add(Conv2D(filters=conv3_filters, kernel_size=3, activation='relu'))\n",
        "  model.add(Conv2D(filters=conv3_filters, kernel_size=3, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2))\n",
        "  model.add(Flatten())\n",
        "  # model.add(Dropout(hp.Float(\"dropout\", min_value=0.2, max_value=0.8, step=0.1)))\n",
        "  model.add(Dense(hp.Int(\"dense_1_units\", 16, 64, 16),activation='relu'))\n",
        "  model.add(Dense(hp.Int(\"dense_2_units\", 16, 64, 16),activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  # learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  learning_rate = 1e-4\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=[tensorflow.keras.metrics.BinaryAccuracy()]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "\n",
        "  # learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  learning_rate = 1e-4\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=[tensorflow.keras.metrics.BinaryAccuracy()]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    TestModel,\n",
        "    objective=\"val_binary_accuracy\",\n",
        "    max_epochs=35,\n",
        "    factor=2,\n",
        "    # max_trials=8,\n",
        "    # executions_per_trial=2,\n",
        "    overwrite=True,\n",
        "    directory=\"gs://tomasmuzasmaster2021/hyperparams\",\n",
        "    project_name=\"DielemanDropoutTest\",\n",
        "    distribution_strategy=strategy\n",
        ")\n",
        "\n",
        "training_dataset, training_steps, validation_dataset, validation_steps = get_dataset()\n",
        "\n",
        "callbacks = [\n",
        "  # BestAccuracyCallback(training_parameters[\"monitor\"], model_name, f\"training_{training + 1}\"),\n",
        "  tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=15, mode='max')\n",
        "]\n",
        "\n",
        "tuner.search(\n",
        "    x= training_dataset,\n",
        "    validation_data = validation_dataset,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    steps_per_epoch = training_steps,\n",
        "    validation_steps = validation_steps,\n",
        "    shuffle=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 115 Complete [00h 05m 22s]\n",
            "val_binary_accuracy: 0.9360193610191345\n",
            "\n",
            "Best val_binary_accuracy So Far: 0.9498172402381897\n",
            "Total elapsed time: 06h 35m 22s\n",
            "\n",
            "Search: Running Trial #116\n",
            "\n",
            "Hyperparameter    |Value             |Best Value So Far \n",
            "conv_1_filters    |16                |16                \n",
            "conv_2_filters    |16                |16                \n",
            "conv_3_filters    |112               |80                \n",
            "dense_1_units     |32                |48                \n",
            "dense_2_units     |48                |48                \n",
            "tuner/epochs      |9                 |35                \n",
            "tuner/initial_e...|5                 |18                \n",
            "tuner/bracket     |4                 |5                 \n",
            "tuner/round       |2                 |5                 \n",
            "tuner/trial_id    |e156fdae9617435...|e10aacffa8520e7...\n",
            "\n",
            "Epoch 6/9\n",
            "  6/247 [..............................] - ETA: 19s - loss: 0.4944 - binary_accuracy: 0.7660WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0032s vs `on_train_batch_end` time: 1.3740s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0032s vs `on_train_batch_end` time: 1.3740s). Check your callbacks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "247/247 [==============================] - 81s 286ms/step - loss: 0.2255 - binary_accuracy: 0.9081 - val_loss: 0.5684 - val_binary_accuracy: 0.7584\n",
            "Epoch 7/9\n",
            "247/247 [==============================] - 88s 356ms/step - loss: 0.1679 - binary_accuracy: 0.9320 - val_loss: 0.3129 - val_binary_accuracy: 0.8850\n",
            "Epoch 8/9\n",
            "247/247 [==============================] - 94s 382ms/step - loss: 0.1541 - binary_accuracy: 0.9380 - val_loss: 0.1588 - val_binary_accuracy: 0.9354\n",
            "Epoch 9/9\n",
            " 75/247 [========>.....................] - ETA: 44s - loss: 0.1475 - binary_accuracy: 0.9403"
          ]
        }
      ]
    }
  ]
}